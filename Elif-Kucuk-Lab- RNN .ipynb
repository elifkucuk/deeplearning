{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-directional LSTM Named Entity Tagger\n",
    "\n",
    "Named-entity recognition (NER) (also known as entity identification, entity chunking and entity extraction) is a subtask of information extraction that seeks to locate and classify named entities in text into pre-defined categories such as the names of persons, organizations, locations, expressions of times, quantities, monetary values, percentages, etc.\n",
    "\n",
    "Most research on NER systems has been structured as taking an unannotated block of text, such as the following **example**:\n",
    "\n",
    "**INPUT:** Jim bought 300 shares of Acme Corp. in 2006.\n",
    "\n",
    "And producing an annotated block of text that highlights the names of entities:\n",
    "\n",
    "**OUTPUT:** [Jim]Person bought 300 shares of [Acme Corp.]Organization in [2006]Time.\n",
    "\n",
    "In this example, a person name consisting of one token, a two-token company name and a temporal expression have been detected and classified.(Wikipedia)\n",
    "\n",
    "Your task in this lab is to implement named entity bi-LSTM based tagger which uses a bi-directional LSTM to extract features from the input sentence, which are then passed through a multi-layer perceptron to predict\n",
    "the tag of the word. Finally, train that model on [WikiNER](https://github.com/neulab/dynet-benchmark/tree/master/data/tags) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set-Up related files and Hyper-parameters\n",
    "using Knet\n",
    "Pkg.installed(\"ArgParse\") == nothing && Pkg.add(\"ArgParse\")\n",
    "using ArgParse\n",
    "include(Pkg.dir(\"Knet\",\"data\",\"wikiner.jl\"));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1. Implement Minibatching\n",
    "Your first task is to implement minibatching. Remember the purpose of minibatching is to feed model with multiple instances in parallel. For example, suppose you are given 2 sentences with a batchsize of 2:<br/>\n",
    "```julia\n",
    "julia> show_batch() # show batch in not implemented in Knet, it is a custom function\n",
    "Inputs sentences:\n",
    "Sent-> That inscribed in the genealogical records of his family is Jiang Zhoutai .\n",
    "NERs-> O    O         O  O   O            O       O  O   O      O  I-PER I-PER   O\n",
    "Sent-> Wallace was a prolific author .\n",
    "NERs-> I-PER   O   O O        O      O\n",
    "\n",
    "Minibatched inputs:\n",
    "Time step 1 ---> Inputs: That  Wallace \n",
    "                 Outputs: O     I-PER\n",
    "Time step 2 ---> Inputs: inscribed  was \n",
    "                 Outputs:O          O\n",
    "Time step 3 ---> Inputs: in  a \n",
    "                 Outputs: O  O\n",
    "Time step 4 ---> Inputs: the  prolific \n",
    "                 Outputs: O  O\n",
    "Time step 5 ---> Inputs: genealogical  author \n",
    "                 Outputs: O              O\n",
    "Time step 6 ---> Inputs: records  . \n",
    "                 Outputs: O       O\n",
    "Time step 7 ---> Inputs: of \n",
    "                 Outputs: O\n",
    "Time step 8 ---> Inputs: his \n",
    "                 Outputs: O\n",
    "Time step 9 ---> Inputs: family \n",
    "                 Outputs: O\n",
    "Time step 10 --->Inputs: is \n",
    "                 Outputs: O\n",
    "Time step 11 ---> Inputs: Jiang \n",
    "                  Outputs: I-PER\n",
    "Time step 12 ---> Inputs: Zhoutai \n",
    "                  Outputs: I-PER\n",
    "Time step 13 ---> Inputs: . \n",
    "                  Outputs: O\n",
    "Batchsizes\n",
    "2 2 2 2 2 2 1 1 1 1 1 1 1\n",
    "```\n",
    "As you see from the previous code snippet, at each time you need to feed your model with **possible** maximum number of words, e.g. at most 2 at least 1 for these 2 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_batch (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You are given make_batches function\n",
    "function make_batches(data, w2i, t2i, batchsize)\n",
    "    batches = []\n",
    "    sorted = sort(data, by=length, rev=true)\n",
    "    for k = 1:batchsize:length(sorted)\n",
    "        lo = k\n",
    "        hi = min(k+batchsize-1, length(sorted))\n",
    "        samples = sorted[lo:hi]\n",
    "        batch = make_batch(samples, w2i, t2i)\n",
    "        push!(batches, batch)\n",
    "    end\n",
    "    return batches\n",
    "end\n",
    "#  You need to implement make_batch function\n",
    "function make_batch(samples, w2i, t2i)\n",
    "    input = Int[]\n",
    "    output = Int[]\n",
    "    longest = length(samples[1])\n",
    "    batchsizes = zeros(Int, longest)\n",
    "    # YOUR ANSWER\n",
    "    for i in 1:longest\n",
    "        batch=0\n",
    "        for j in 1:length(samples)\n",
    "            lengthSentence=length(samples[j])\n",
    "            if lengthSentence>=i\n",
    "                indx=get(w2i,samples[j][i][1],w2i[UNK])\n",
    "                push!(input,indx)\n",
    "                indx=get(t2i,samples[j][i][2],t2i[\"O\"])\n",
    "                push!(output,indx)\n",
    "                batch+=1\n",
    "            end \n",
    "        end\n",
    "        batchsizes[i]=batch\n",
    "    end\n",
    "    return input, output, batchsizes\n",
    "end"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Problem 2. Implement model initialization\n",
    "You need to implement initmodel function which takes input, hidden and output dimensions and returns the whole model as `Array{Any}` julia data type.\n",
    "Hints: You may use Knet's ```rnninit``` documentation, remember our rnn is bidirectional LSTM.\n",
    "```julia\n",
    "julia> @doc rnninit\n",
    "  rnninit(inputSize, hiddenSize; opts...)\n",
    "\n",
    "  Return an (r,w) pair where r is a RNN struct and w is a single weight array that includes all matrices and biases for the RNN. Keyword arguments:\n",
    "\n",
    "    •    rnnType=:lstm Type of RNN: One of :relu, :tanh, :lstm, :gru.\n",
    "\n",
    "    •    numLayers=1: Number of RNN layers.\n",
    "\n",
    "    •    bidirectional=false: Create a bidirectional RNN if true.\n",
    "\n",
    "    •    dropout=0.0: Dropout probability. Ignored if numLayers==1.\n",
    "\n",
    "    •    skipInput=false: Do not multiply the input with a matrix if true.\n",
    "\n",
    "    •    dataType=Float32: Data type to use for weights.\n",
    "\n",
    "    •    algo=0: Algorithm to use, see CUDNN docs for details.\n",
    "\n",
    "    •    seed=0: Random number seed. Uses time() if 0.\n",
    "\n",
    "    •    winit=xavier: Weight initialization method for matrices.\n",
    "\n",
    "    •    binit=zeros: Weight initialization method for bias vectors.\n",
    "\n",
    "    •    `usegpu=(gpu()>=0): GPU used by default if one exists.\n",
    "\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initweights (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w[1]   => weight/bias params for forward LSTM network\n",
    "# w[2:5] => weight/bias params for MLP+softmax network\n",
    "# w[6]   => word embeddings\n",
    "# w[7]   => rnnstruct given by rnninit function\n",
    "# Hint you mave take a look main function below to better understand its calling convention\n",
    "function initweights(atype, hidden, words, tags, embed, mlp, usegpu, winit=0.01)\n",
    "    w = Array{Any}(7)\n",
    "    # YOUR ANSWER\n",
    "    w[6]=atype(randn(embed,words).*winit)\n",
    "    w[7],w[1] = rnninit(embed,hidden; rnnType=:lstm , bidirectional=true)\n",
    "    w[2]=atype(randn(mlp,2*hidden).*winit)\n",
    "    w[3]=atype(zeros(mlp,1))\n",
    "    w[4]=atype(randn(tags,mlp).*winit)\n",
    "    w[5]=atype(zeros(tags,1)) \n",
    "    #println(\"embed\", embed)\n",
    "    #println(\"hidden\", hidden)\n",
    "    #println(\"words \", words)\n",
    "    #println(\"size of w1 \", size(w[1]))\n",
    "     return w\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3. Implement Predict function\n",
    "Returns scores for output tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predict (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function predict(ws, xs, batchsizes)\n",
    "    # YOUR ANSWER\n",
    "    w_r,w_mlp,b_mlp,w_y,b_y,wx,r = ws\n",
    "    x = wx[:,xs] # xs is embedding that we init randomly, we are finding the x that corresponds the one column in embedd\n",
    "    (y,_ )= rnnforw(r,w_r,x,batchSizes=batchsizes) # y=(H,ΣBt)\n",
    "    y=sigm.(w_mlp*y.+b_mlp)    \n",
    "    return w_y * y .+ b_y  # return=(V,ΣBt)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4. Implement Loss function\n",
    "That function basically takes the predictions and returns the negative log-likelihood of these predictions as loss.\n",
    "Hint: You may have a look Knet's ```nll``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loss (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# our loss function\n",
    "function loss(w, x, ygold, batchsizes)\n",
    "    return nll(predict(w,x,batchsizes),ygold)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(::gradfun) (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lossgradient = gradloss(loss) # Knet's automatic gradient calculator function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5. Implemlent Train function\n",
    "That function takes the model(w), input(x) and correct labels(ygold)\n",
    "and updates the model(w) parameters.\n",
    "**Hints**\n",
    "Remember lossgradient function returns the loss value and gradients as 2 arguments:\n",
    "```\n",
    "gradients_of_loss, lossval = lossgradient(.....)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train! (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train!(w, x, ygold, batchsizes, opt)\n",
    "    # YOUR ANSWER\n",
    "    gradients_of_loss, lossval=lossgradient(w,x,ygold,batchsizes)\n",
    "    update!(w,gradients_of_loss,opt)\n",
    "   \n",
    "    return lossval\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 6. Implement Accuracy function\n",
    "Accuracy function counts the number of words(tokens) and also counts the number of correctly predicted tokens and returns ```number_of_correctly_pred_token / number_of_total_token```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tagval (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function tagval(a)\n",
    "    m = maximum(a,1)\n",
    "    b=Array{Any}(size(a,2))\n",
    "    for j in 1:size(a,2)\n",
    "        for i in 1:size(a,1)\n",
    "            if m[j]==a[i,j]\n",
    "                b[j] = i\n",
    "            end \n",
    "        end\n",
    "    end\n",
    "    return b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function accuracy(w, batches, i2t)\n",
    "    # YOUR ANSWER\n",
    "    correct_num=0\n",
    "    token_num=0\n",
    "    for (k,batch) in enumerate(batches)\n",
    "        x,ygold,batchSizes=batch \n",
    "        token_num+= sum(batchSizes)\n",
    "\n",
    "        preds=predict(w,x,batchSizes)\n",
    "        preds=convert(Array,preds) \n",
    "        maxval = maximum(preds,1)\n",
    "        ypred=tagval(preds)\n",
    "        for i in 1:length(ygold)\n",
    "            if(ygold[i]==ypred[i])\n",
    "                correct_num+=1\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    tag_acc=correct_num/token_num   \n",
    "    return tag_acc\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 1 method)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Do not touch this function\n",
    "function main(args)\n",
    "    s = ArgParseSettings()\n",
    "    s.description = \"Bidirectional LSTM Tagger in Knet\"\n",
    "\n",
    "    @add_arg_table s begin\n",
    "        (\"--usegpu\"; action=:store_true; help=\"use GPU or not\")\n",
    "        (\"--embed\"; arg_type=Int; default=128; help=\"word embedding size\")\n",
    "        (\"--hidden\"; arg_type=Int; default=50; help=\"LSTM hidden size\")\n",
    "        (\"--mlp\"; arg_type=Int; default=32; help=\"MLP size\")\n",
    "        (\"--epochs\"; arg_type=Int; default=20; help=\"number of training epochs\")\n",
    "        (\"--minoccur\"; arg_type=Int; default=6; help=\"word min occurence limit\")\n",
    "        (\"--report\"; arg_type=Int; default=500; help=\"report period in iters\")\n",
    "        (\"--valid\"; arg_type=Int; default=10000; help=\"valid period in iters\")\n",
    "        (\"--seed\"; arg_type=Int; default=-1; help=\"random seed\")\n",
    "        (\"--batchsize\"; arg_type=Int; default=100; help=\"batchsize\")\n",
    "    end\n",
    "\n",
    "    isa(args, AbstractString) && (args=split(args))\n",
    "    o = parse_args(args, s; as_symbols=true)\n",
    "    o[:seed] > 0 && Knet.setseed(o[:seed])\n",
    "    atype = o[:atype] = !o[:usegpu] ? Array{Float32} : KnetArray{Float32}\n",
    "    #datadir = abspath(joinpath(@__DIR__, \"../data/tags\"))\n",
    "    datadir = WIKINER_DIR\n",
    "\n",
    "    # load WikiNER data\n",
    "    data = WikiNERData(datadir, o[:minoccur])\n",
    "\n",
    "    # build model\n",
    "    nwords, ntags = length(data.w2i), data.ntags\n",
    "    w = initweights(\n",
    "        atype, o[:hidden], nwords, ntags,  o[:embed], o[:mlp], o[:usegpu])\n",
    "    opt = optimizers(w, Adam)\n",
    "\n",
    "    # make batches\n",
    "    trn = make_batches(data.trn, data.w2i, data.t2i, o[:batchsize])\n",
    "    dev = make_batches(data.dev, data.w2i, data.t2i, o[:batchsize])\n",
    "\n",
    "    # train bilstm tagger\n",
    "    nwords = data.nwords\n",
    "    println(\"nwords=$nwords, ntags=$ntags\"); flush(STDOUT)\n",
    "    println(\"startup time: \", Int((now()-t00).value)*0.001); flush(STDOUT)\n",
    "    t0 = now()\n",
    "    all_time = dev_time = all_tagged = this_tagged = this_loss = 0\n",
    "    for epoch = 1:o[:epochs]\n",
    "        # training\n",
    "        shuffle!(trn)\n",
    "        for (k,batch) in enumerate(trn)\n",
    "            x, ygold, batchsizes = batch\n",
    "            num_tokens = sum(batchsizes)\n",
    "            batch_loss = train!(w, x, ygold,  batchsizes, opt)\n",
    "            this_loss += num_tokens*batch_loss\n",
    "            this_tagged += num_tokens\n",
    "        end\n",
    "\n",
    "        # validation\n",
    "        dev_start = now()\n",
    "        tag_acc = accuracy(w, dev, data.i2t)\n",
    "        dev_time += Int((now()-dev_start).value)*0.001\n",
    "        train_time = Int((now()-t0).value)*0.001-dev_time\n",
    "\n",
    "        # report\n",
    "        @printf(\"epoch %d finished, loss=%f\\n\", epoch, this_loss/this_tagged)\n",
    "        all_tagged += this_tagged\n",
    "        this_loss = this_tagged = 0\n",
    "        all_time = Int((now()-t0).value)*0.001\n",
    "        @printf(\"tag_acc=%.4f, time=%.4f, word_per_sec=%.4f\\n\",\n",
    "                tag_acc, train_time, all_tagged/train_time)\n",
    "        flush(STDOUT)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nwords=119101, ntags=9\n",
      "startup time: 15.527000000000001\n",
      "epoch 1 finished, loss=0.418017\n",
      "tag_acc=0.8485, time=84.7930, word_per_sec=41272.3456\n",
      "epoch 2 finished, loss=0.222018\n",
      "tag_acc=0.9043, time=163.8070, word_per_sec=42728.4060\n",
      "epoch 3 finished, loss=0.130626\n",
      "tag_acc=0.9258, time=247.6100, word_per_sec=42400.6219\n",
      "epoch 4 finished, loss=0.093117\n",
      "tag_acc=0.9314, time=333.6950, word_per_sec=41949.7565\n",
      "epoch 5 finished, loss=0.075671\n",
      "tag_acc=0.9327, time=421.1340, word_per_sec=41549.7918\n",
      "epoch 6 finished, loss=0.064969\n",
      "tag_acc=0.9296, time=501.9500, word_per_sec=41832.1267\n",
      "epoch 7 finished, loss=0.056741\n",
      "tag_acc=0.9302, time=593.2460, word_per_sec=41293.5646\n",
      "epoch 8 finished, loss=0.049628\n",
      "tag_acc=0.9296, time=679.6440, word_per_sec=41193.4013\n",
      "epoch 9 finished, loss=0.043440\n",
      "tag_acc=0.9263, time=762.2220, word_per_sec=41321.8905\n"
     ]
    }
   ],
   "source": [
    "t00 = now();main(\"--usegpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
