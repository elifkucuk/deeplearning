{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Please refer http://ufldl.stanford.edu/tutorial/supervised/SoftmaxRegression/ for more detailed explanation\n",
    "for p in (\"Compat\",\"GZip\")\n",
    "    Pkg.installed(p) == nothing && Pkg.add(p)\n",
    "end\n",
    "using Compat, GZip, DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "loaddata (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loaddata()\n",
    "    info(\"Loading MNIST...\")\n",
    "    xtrn = gzload(\"train-images-idx3-ubyte.gz\")[17:end]\n",
    "    xtst = gzload(\"t10k-images-idx3-ubyte.gz\")[17:end]\n",
    "    ytrn = gzload(\"train-labels-idx1-ubyte.gz\")[9:end]\n",
    "    ytst = gzload(\"t10k-labels-idx1-ubyte.gz\")[9:end]\n",
    "    return (xtrn, ytrn, xtst, ytst)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gzload (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function gzload(file; path=\"$file\", url=\"http://yann.lecun.com/exdb/mnist/$file\")\n",
    "    isfile(path) || download(url, path)\n",
    "    f = gzopen(path)\n",
    "    a = @compat read(f)\n",
    "    close(f)\n",
    "    return(a)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minibatch (generic function with 2 methods)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function minibatch(X, Y, bs=100)\n",
    "    #takes raw input (X) and gold labels (Y)\n",
    "    #returns list of minibatches (x, y)\n",
    "    data = Any[]\n",
    "    #start of step 1\n",
    "    for i=1:bs:size(X, 2)\n",
    "        bl = min(i+bs-1, size(X, 2))\n",
    "        push!(data, (X[:, i:bl], Y[:, i:bl]))\n",
    "    end\n",
    "    #end of step 1\n",
    "    return data\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_params (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function init_params(ninputs, noutputs)\n",
    "    # takes number of inputs and number of outputs(number of classes)\n",
    "    # returns randomly generated W  with zero mean\n",
    "    # and 0.001 std and b(must be zeros vector) params of softmax\n",
    "    # start of step 2\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    w=randn(noutputs,ninputs)*0.001\n",
    "    b=zeros(noutputs,1)\n",
    "    \n",
    "    return w,b\n",
    "    #println(size(w))\n",
    "    #end of step 2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_forw (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax_forw(W, b, data)\n",
    "    #applies affine transformation and softmax function\n",
    "    #returns predicted probabilities\n",
    "\n",
    "    ### step 3\n",
    "    # YOUR CODE HERE\n",
    "    pred=(W*data.+b)\n",
    "    #p=pred./sum(pred,1)\n",
    "    p=zeros(size(pred))\n",
    "    for i=1:size(data,2)\n",
    "        p[:,i]=exp(pred[:,i])./sum(exp(pred[:,i]))\n",
    "    end\n",
    "    return p\n",
    "    ### step 3\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_cost (generic function with 1 method)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function softmax_cost(W, b, data, labels)\n",
    "    #takes W, b paremeters, data and correct labels\n",
    "    #calculates the soft loss, gradient of w and gradient of b\n",
    "\n",
    "    #start of step 3\n",
    "    # YOUR CODE HERE\n",
    "    p=softmax_forw(W,b,data)\n",
    "    n=size(data,2)\n",
    "    #println(size(p))\n",
    "    #println(size(labels))\n",
    "    grad_y=(labels-p)\n",
    "    grad_w=(grad_y*data')./n\n",
    "    grad_b=sum(grad_y,2)./n\n",
    "    \n",
    "    \n",
    "    #println(p)\n",
    "    \n",
    "    loss=-sum(labels.*log(p))/n\n",
    "    #println(size(loss))\n",
    "    return (loss,grad_w,grad_b)\n",
    "    \n",
    "    #end of step 3\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function accuracy(ygold, yhat)\n",
    "    correct = 0.0\n",
    "    for i=1:size(ygold, 2)\n",
    "        correct += indmax(ygold[:,i]) == indmax(yhat[:, i]) ? 1.0 : 0.0\n",
    "    end\n",
    "    return correct / size(ygold, 2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grad_check (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function grad_check(W, b, data, labels)\n",
    "    function numeric_gradient()\n",
    "        epsilon = 0.0001\n",
    "        gw = zeros(size(W))\n",
    "        gb = zeros(size(b))\n",
    "        println(\"weight size\", gw)\n",
    "        println(\"bias size\", gb)\n",
    "        #p=softmax_forw(W,b,data)\n",
    "        #n=size(data,2)\n",
    "        #loss=-sum(labels.*log(p))/n\n",
    "        loss,_,_= softmax_cost(W,b,data,labels)\n",
    "        #start of step 4\n",
    "        # YOUR CODE HERE\n",
    "        for i = 1 : size(W,1)\n",
    "            for j = 1 : size(W,2)\n",
    "                W[i,j]=W[i,j]+epsilon\n",
    "                loss_gw,_,_= softmax_cost(W,b,data,labels)\n",
    "                gw[i,j]=(loss_gw-loss)/epsilon\n",
    "                W[i,j]=W[i,j]-epsilon\n",
    "                #p_gw=softmax_forw(W,b,data)\n",
    "                #loss_gw=-sum(labels.*log(p_gw))/n\n",
    "                \n",
    "                #p_gb=softmax_forw(W,(b.+epsilon),data)\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        for i = 1:size(b,1)\n",
    "            b[i]=b[i]+epsilon\n",
    "            loss_gb,_,_=softmax_cost(W,b,data,labels)\n",
    "            gb[k] = (loss_gb-loss)/epsilon \n",
    "            b[i]=b[i]-epsilon\n",
    "        end \n",
    "        \n",
    "        #end of step 4\n",
    "\n",
    "        return gw, gb\n",
    "    end\n",
    "\n",
    "    cost,gradW,gradB = softmax_cost(W, b, data, labels)\n",
    "    gw, gb = numeric_gradient()\n",
    "\n",
    "    #println(size(gradW))\n",
    "    #println(size(gw))\n",
    "    #println(size(gradB))\n",
    "    #println(size(gb))\n",
    "    diff = sqrt(sum((gradW - gw) .^ 2) + sum((gradB - gb) .^ 2))\n",
    "    println(\"Diff: $diff\")\n",
    "    if diff < 1e-7\n",
    "        println(\"Gradient Checking Passed\")\n",
    "    else\n",
    "        println(\"Diff must be < 1e-7\")\n",
    "    end\n",
    "    flush(STDOUT)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 2 methods)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function train(W, b, data, lr=0.15)\n",
    "    totalcost = 0.0\n",
    "    numins = 0\n",
    "    for (x, y) in data\n",
    "        ### step 5\n",
    "        # YOUR CODE HERE\n",
    "        # Get the cost and gradients\n",
    "        # Update parameters using gradients\n",
    "        cost,grad_w,grad_b=softmax_cost(W, b, x, y)\n",
    "        W  .= W + lr * grad_w\n",
    "        #println(size(grad_b))\n",
    "        #println(size(b))\n",
    "\n",
    "        b  .= b + lr * grad_b\n",
    "\n",
    "        ### step 5\n",
    "        totalcost += cost * size(x, 2)\n",
    "        numins += size(x, 2)\n",
    "    end\n",
    "    avgcost = totalcost / numins\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function main()\n",
    "    srand(12345)\n",
    "    # Size of input vector (MNIST images are 28x28)\n",
    "    ninputs = 28 * 28\n",
    "\n",
    "    # Number of classes (MNIST images fall into 10 classes)\n",
    "    noutputs = 10\n",
    "\n",
    "    ## Data loading & preprocessing\n",
    "    #\n",
    "    #  In this section, we load the input and output data,\n",
    "    #  prepare data to feed into softmax model.\n",
    "    #  For softmax regression on MNIST pixels,\n",
    "    #  the input data is the images, and\n",
    "    #  the output data is the labels.\n",
    "    #  Size of xtrn: (28,28,1,60000)\n",
    "    #  Size of xtrn must be: (784, 60000)\n",
    "    #  Size of xtst must be: (784, 10000)\n",
    "\n",
    "    xtrnraw, ytrnraw, xtstraw, ytstraw = loaddata()\n",
    "    info(\"data loaded\");flush(STDOUT)\n",
    "    xtrn = convert(Array{Float32}, reshape(xtrnraw ./ 255, 28*28, div(length(xtrnraw), 784)))\n",
    "    ytrnraw[ytrnraw.==0]=10;\n",
    "    ytrn = convert(Array{Float32}, sparse(convert(Vector{Int},ytrnraw),1:length(ytrnraw),one(eltype(ytrnraw)),10,length(ytrnraw)))\n",
    "\n",
    "    xtst = convert(Array{Float32}, reshape(xtstraw ./ 255, 28*28, div(length(xtstraw), 784)))\n",
    "    ytstraw[ytstraw.==0]=10;\n",
    "    ytst = convert(Array{Float32}, sparse(convert(Vector{Int},ytstraw),1:length(ytstraw),one(eltype(ytstraw)),10,length(ytstraw)))\n",
    "    info(\"size(xtrn):\",size(xtrn),\" size(ytrn):\",size(ytrn))\n",
    "    info(\"size(xttst):\",size(xtst),\"size(ytst):\",size(ytst))\n",
    "    flush(STDOUT)\n",
    "    ## STEP 1: Create minibatches\n",
    "    # It takes the input matrix (X) and gold labels (Y)\n",
    "    # returns list of tuples contain minibatched input and labels (x, y)\n",
    "    # For only LAB-02, we provide you a complete minibatch function\n",
    "    # Please try to understand what it does before moving further\n",
    "    # since you are going to write your own batch functions in upcoming\n",
    "    # lab sessions.\n",
    "    bs = 100\n",
    "    trn_data = minibatch(xtrn, ytrn, bs)\n",
    "    info(\"minibatches ready\");flush(STDOUT)\n",
    "    info(\"length trn_data:\",length(trn_data))\n",
    "    info(\"size(trn_data[1][1]):\",size(trn_data[1][1]),\" size(trn_data[1][2]):\",size(trn_data[1][2]))\n",
    "    flush(STDOUT)    \n",
    "    ## STEP 2: Initialize parameters\n",
    "    #  Complete init_params function\n",
    "    #  It takes number of inputs and number of outputs(number of classes)\n",
    "    #  It returns randomly generated W matrix with\n",
    "    # mean 0 and standard deviation 0.001\n",
    "    #  and zeros bias vector\n",
    "    W, b = init_params(ninputs, noutputs)\n",
    "    info(\"parameters are set\");flush(STDOUT)\n",
    "    ## STEP 3: Implement softmax_forw and softmax_cost\n",
    "    #  softmax_forw function takes W, b, and data\n",
    "    #  calculates predicted probabilities\n",
    "    #\n",
    "    #  softmax_cost function obtains probabilites by calling softmax_forw\n",
    "    #  then calculates soft loss and\n",
    "    #  gradient of W and gradient of b\n",
    "\n",
    "    ## STEP 4: Gradient checking\n",
    "    #  Skip this part for the lab session, but complete later.\n",
    "    #  As with any learning algorithm, you should always check that your\n",
    "    #  gradients are correct before learning the parameters.\n",
    "\n",
    "    debug = false #Turn this parameter off, after gradient checking passed\n",
    "\n",
    "    if debug\n",
    "        info(\"debugging mode\");flush(STDOUT)\n",
    "        grad_check(W, b, xtrn[:, 1:100], ytrn[:, 1:100])\n",
    "    end\n",
    "\n",
    "    lr = 0.15\n",
    "\n",
    "    ## STEP 5: Training\n",
    "    #  The train function takes model parameters and the data\n",
    "    #  Trains the model over minibatches\n",
    "    #  For each minibatch, cost and gradients are calculated then model parameters updated\n",
    "    #  train function returns the average cost\n",
    "\n",
    "    for i=1:50\n",
    "        cost = train(W, b, trn_data, lr)\n",
    "        pred = softmax_forw(W, b, xtrn)\n",
    "        trnacc = accuracy(ytrn, pred)\n",
    "        pred = softmax_forw(W, b, xtst)\n",
    "        tstacc = accuracy(ytst, pred)\n",
    "        @printf(\"epoch: %d softloss: %g trn accuracy: %g tst accuracy: %g\\n\", i, cost, trnacc, tstacc)\n",
    "        flush(STDOUT)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mLoading MNIST...\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mdata loaded\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36msize(xtrn):(784, 60000) size(ytrn):(10, 60000)\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36msize(xttst):(784, 10000)size(ytst):(10, 10000)\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mminibatches ready\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mlength trn_data:600\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36msize(trn_data[1][1]):(784, 100) size(trn_data[1][2]):(10, 100)\n",
      "\u001b[39m\u001b[1m\u001b[36mINFO: \u001b[39m\u001b[22m\u001b[36mparameters are set\n",
      "\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 softloss: 0.481559 trn accuracy: 0.896983 tst accuracy: 0.9064\n",
      "epoch: 2 softloss: 0.339105 trn accuracy: 0.907617 tst accuracy: 0.9119\n",
      "epoch: 3 softloss: 0.31604 trn accuracy: 0.912017 tst accuracy: 0.9142\n",
      "epoch: 4 softloss: 0.303876 trn accuracy: 0.914783 tst accuracy: 0.9156\n",
      "epoch: 5 softloss: 0.29597 trn accuracy: 0.916567 tst accuracy: 0.9172\n",
      "epoch: 6 softloss: 0.290259 trn accuracy: 0.918033 tst accuracy: 0.9187\n",
      "epoch: 7 softloss: 0.285858 trn accuracy: 0.919233 tst accuracy: 0.9198\n",
      "epoch: 8 softloss: 0.282317 trn accuracy: 0.920083 tst accuracy: 0.92\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "\u001b[91mInterruptException:\u001b[39m",
     "output_type": "error",
     "traceback": [
      "\u001b[91mInterruptException:\u001b[39m",
      "",
      "Stacktrace:",
      " [1] \u001b[1mntuple\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Base.##32#33{Tuple{Base.OneTo{Int64},Base.OneTo{Int64}},Int64}, ::Int64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./tuple.jl:108\u001b[22m\u001b[22m",
      " [2] \u001b[1mgetindex\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}, ::UnitRange{Int64}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./range.jl:162\u001b[22m\u001b[22m",
      " [3] \u001b[1mreduced_indices\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Tuple{Base.OneTo{Int64},Base.OneTo{Int64}}, ::Int64, ::Base.OneTo{Int64}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./reducedim.jl:16\u001b[22m\u001b[22m",
      " [4] \u001b[1mreducedim_initarray\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,2}, ::Int64, ::Float64, ::Type{Float64}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./reducedim.jl:73\u001b[22m\u001b[22m",
      " [5] \u001b[1mmapreducedim\u001b[22m\u001b[22m at \u001b[1m./reducedim.jl:242\u001b[22m\u001b[22m [inlined]",
      " [6] \u001b[1msum\u001b[22m\u001b[22m at \u001b[1m./reducedim.jl:583\u001b[22m\u001b[22m [inlined]",
      " [7] \u001b[1msum\u001b[22m\u001b[22m at \u001b[1m./reducedim.jl:585\u001b[22m\u001b[22m [inlined]",
      " [8] \u001b[1msoftmax_cost\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,2}, ::Array{Float64,2}, ::Array{Float32,2}, ::Array{Float32,2}\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[8]:13\u001b[22m\u001b[22m",
      " [9] \u001b[1mtrain\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::Array{Float64,2}, ::Array{Float64,2}, ::Array{Any,1}, ::Float64\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[13]:9\u001b[22m\u001b[22m",
      " [10] \u001b[1mmain\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./In[14]:82\u001b[22m\u001b[22m",
      " [11] \u001b[1minclude_string\u001b[22m\u001b[22m\u001b[1m(\u001b[22m\u001b[22m::String, ::String\u001b[1m)\u001b[22m\u001b[22m at \u001b[1m./loading.jl:522\u001b[22m\u001b[22m"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example experiment log:\n",
    "#===========================\n",
    "Diff: 1.8302437625092545e-9\n",
    "Gradient Checking Passed\n",
    "epoch: 1 softloss: 0.481559 trn accuracy: 0.896983 tst accuracy: 0.9064\n",
    "epoch: 2 softloss: 0.339105 trn accuracy: 0.907617 tst accuracy: 0.9119\n",
    "epoch: 3 softloss: 0.31604 trn accuracy: 0.912017 tst accuracy: 0.9142\n",
    "epoch: 4 softloss: 0.303876 trn accuracy: 0.914783 tst accuracy: 0.9156\n",
    "epoch: 5 softloss: 0.29597 trn accuracy: 0.916567 tst accuracy: 0.9172\n",
    "epoch: 6 softloss: 0.290259 trn accuracy: 0.918033 tst accuracy: 0.9187\n",
    "epoch: 7 softloss: 0.285858 trn accuracy: 0.919233 tst accuracy: 0.9198\n",
    "epoch: 8 softloss: 0.282317 trn accuracy: 0.920083 tst accuracy: 0.92\n",
    "epoch: 9 softloss: 0.279378 trn accuracy: 0.9209 tst accuracy: 0.9204\n",
    "epoch: 10 softloss: 0.276879 trn accuracy: 0.921717 tst accuracy: 0.9211\n",
    "epoch: 11 softloss: 0.274716 trn accuracy: 0.92225 tst accuracy: 0.9207\n",
    "epoch: 12 softloss: 0.272816 trn accuracy: 0.92305 tst accuracy: 0.9214\n",
    "epoch: 13 softloss: 0.271127 trn accuracy: 0.923667 tst accuracy: 0.9214\n",
    "epoch: 14 softloss: 0.269609 trn accuracy: 0.924133 tst accuracy: 0.9215\n",
    "epoch: 15 softloss: 0.268235 trn accuracy: 0.924417 tst accuracy: 0.922\n",
    "epoch: 16 softloss: 0.26698 trn accuracy: 0.9247 tst accuracy: 0.9219\n",
    "epoch: 17 softloss: 0.265828 trn accuracy: 0.924933 tst accuracy: 0.9218\n",
    "epoch: 18 softloss: 0.264764 trn accuracy: 0.92505 tst accuracy: 0.922\n",
    "epoch: 19 softloss: 0.263777 trn accuracy: 0.925367 tst accuracy: 0.9223\n",
    "epoch: 20 softloss: 0.262856 trn accuracy: 0.92575 tst accuracy: 0.9225\n",
    "epoch: 21 softloss: 0.261995 trn accuracy: 0.9263 tst accuracy: 0.9227\n",
    "epoch: 22 softloss: 0.261186 trn accuracy: 0.926567 tst accuracy: 0.9226\n",
    "epoch: 23 softloss: 0.260424 trn accuracy: 0.9269 tst accuracy: 0.9229\n",
    "epoch: 24 softloss: 0.259704 trn accuracy: 0.92715 tst accuracy: 0.9227\n",
    "epoch: 25 softloss: 0.259022 trn accuracy: 0.927367 tst accuracy: 0.9227\n",
    "epoch: 26 softloss: 0.258374 trn accuracy: 0.9275 tst accuracy: 0.9229\n",
    "epoch: 27 softloss: 0.257758 trn accuracy: 0.927767 tst accuracy: 0.923\n",
    "epoch: 28 softloss: 0.257171 trn accuracy: 0.928083 tst accuracy: 0.9229\n",
    "epoch: 29 softloss: 0.25661 trn accuracy: 0.92825 tst accuracy: 0.9231\n",
    "epoch: 30 softloss: 0.256073 trn accuracy: 0.92835 tst accuracy: 0.9229\n",
    "epoch: 31 softloss: 0.255558 trn accuracy: 0.928517 tst accuracy: 0.923\n",
    "epoch: 32 softloss: 0.255064 trn accuracy: 0.928783 tst accuracy: 0.9228\n",
    "epoch: 33 softloss: 0.254589 trn accuracy: 0.92895 tst accuracy: 0.9229\n",
    "epoch: 34 softloss: 0.254133 trn accuracy: 0.9291 tst accuracy: 0.9227\n",
    "epoch: 35 softloss: 0.253692 trn accuracy: 0.929167 tst accuracy: 0.9228\n",
    "epoch: 36 softloss: 0.253268 trn accuracy: 0.92925 tst accuracy: 0.9227\n",
    "epoch: 37 softloss: 0.252858 trn accuracy: 0.929417 tst accuracy: 0.923\n",
    "epoch: 38 softloss: 0.252462 trn accuracy: 0.929567 tst accuracy: 0.9229\n",
    "epoch: 39 softloss: 0.252078 trn accuracy: 0.929667 tst accuracy: 0.9228\n",
    "epoch: 40 softloss: 0.251707 trn accuracy: 0.929783 tst accuracy: 0.9229\n",
    "epoch: 41 softloss: 0.251347 trn accuracy: 0.929867 tst accuracy: 0.9231\n",
    "epoch: 42 softloss: 0.250998 trn accuracy: 0.930067 tst accuracy: 0.9235\n",
    "epoch: 43 softloss: 0.25066 trn accuracy: 0.9301 tst accuracy: 0.9235\n",
    "epoch: 44 softloss: 0.250331 trn accuracy: 0.930233 tst accuracy: 0.9235\n",
    "epoch: 45 softloss: 0.250011 trn accuracy: 0.930333 tst accuracy: 0.9235\n",
    "epoch: 46 softloss: 0.2497 trn accuracy: 0.9305 tst accuracy: 0.9237\n",
    "epoch: 47 softloss: 0.249397 trn accuracy: 0.930583 tst accuracy: 0.9238\n",
    "epoch: 48 softloss: 0.249102 trn accuracy: 0.9307 tst accuracy: 0.9239\n",
    "epoch: 49 softloss: 0.248815 trn accuracy: 0.93085 tst accuracy: 0.9242\n",
    "epoch: 50 softloss: 0.248535 trn accuracy: 0.930933 tst accuracy: 0.9243\n",
    "============================#\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
